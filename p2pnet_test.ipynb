{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHSuGBW4Bjl5"
      },
      "source": [
        "# P2PNet Inference Notebook\n",
        "This notebook's purpose was to try to load the out of the box P2P model, and later try to use the concept implemented in that paper into the baseline MCNN.\n",
        "However, due to difficulties on the implementation, and the low expectations we had for this approach, we ended up abandoning it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmS8mkS2Bjl-"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5PZoJkrBjmA",
        "outputId": "259a2d19-12d3-49b3-cb7f-42707e32f514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/tthien/shanghaitech?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 333M/333M [00:03<00:00, 92.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/tthien/shanghaitech/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"tthien/shanghaitech\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBEWSEQiBjmC",
        "outputId": "4740c4d7-f930-4965-9704-3600cbe6455b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.12/dist-packages (1.13)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboardX) (25.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.12/dist-packages (from tensorboardX) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.4\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision tensorboardX easydict pandas numpy scipy matplotlib Pillow opencv-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14vrjVXFBjmD"
      },
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBYOda5eBjmD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# TODO: Change this to the path of your project in Google Drive\n",
        "#PROJECT_PATH = '/content/drive/MyDrive/CrowdCounting-P2PNet'\n",
        "\n",
        "import sys\n",
        "# sys.path.append(PROJECT_PATH)\n",
        "\n",
        "# Path to the folder containing images\n",
        "#IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
        "IMAGE_PATH = \"/root/.cache/kagglehub/datasets/tthien/shanghaitech/versions/1/ShanghaiTech/part_A/train_data/images\"\n",
        "\n",
        "# Path to the folder containing ground truth files\n",
        "# GT_PATH = os.path.join(PROJECT_PATH, 'gt')\n",
        "GT_PATH = \"/root/.cache/kagglehub/datasets/tthien/shanghaitech/versions/1/ShanghaiTech/part_A/train_data/ground-truth\"\n",
        "\n",
        "# Path to the model weights\n",
        "#WEIGHTS_PATH = os.path.join(PROJECT_PATH, 'weights/SHTechA.pth')\n",
        "WEIGHTS_PATH = \"/content/SHTechA.pth\"\n",
        "# Device to run the model on\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TROD5gDxBjmE"
      },
      "source": [
        "## 3. Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXUpuvF4BjmE"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "from easydict import EasyDict as edict\n",
        "from typing import Optional, List\n",
        "from torch import Tensor\n",
        "\n",
        "# Misc utils\n",
        "class NestedTensor(object):\n",
        "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
        "        self.tensors = tensors\n",
        "        self.mask = mask\n",
        "\n",
        "    def to(self, device):\n",
        "        # type: (Device) -> NestedTensor # noqa\n",
        "        cast_tensor = self.tensors.to(device)\n",
        "        mask = self.mask\n",
        "        if mask is not None:\n",
        "            assert mask is not None\n",
        "            cast_mask = mask.to(device)\n",
        "        else:\n",
        "            cast_mask = None\n",
        "        return NestedTensor(cast_tensor, cast_mask)\n",
        "\n",
        "    def decompose(self):\n",
        "        return self.tensors, self.mask\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.tensors)\n",
        "\n",
        "\n",
        "def _max_by_axis_pad(the_list):\n",
        "    maxes = the_list[0]\n",
        "    for sublist in the_list[1:]:\n",
        "        for index, item in enumerate(sublist):\n",
        "            maxes[index] = max(maxes[index], item)\n",
        "\n",
        "    block = 128\n",
        "\n",
        "    for i in range(2):\n",
        "        maxes[i+1] = ((maxes[i+1] - 1) // block + 1) * block\n",
        "    return maxes\n",
        "\n",
        "\n",
        "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
        "    if tensor_list[0].ndim == 3:\n",
        "        max_size = _max_by_axis_pad([list(img.shape) for img in tensor_list])\n",
        "        batch_shape = [len(tensor_list)] + max_size\n",
        "        b, c, h, w = batch_shape\n",
        "        dtype = tensor_list[0].dtype\n",
        "        device = tensor_list[0].device\n",
        "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
        "        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
        "        for img, pad_img, m in zip(tensor_list, tensor, mask):\n",
        "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
        "            m[: img.shape[1], :img.shape[2]] = False\n",
        "    else:\n",
        "        raise ValueError('not supported')\n",
        "    return NestedTensor(tensor, mask)\n",
        "\n",
        "\n",
        "# VGG Backbone\n",
        "model_urls = {\n",
        "    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, features, num_classes=1000, init_weights=True):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def make_layers(cfg, batch_norm=False, sync=False):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "cfgs = {\n",
        "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "def _vgg(arch, cfg, batch_norm, pretrained, progress, **kwargs):\n",
        "    if pretrained:\n",
        "        kwargs['init_weights'] = False\n",
        "    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vgg16_bn(pretrained=False, progress=True, **kwargs):\n",
        "    return _vgg('vgg16_bn', 'D', True, pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "class BackboneBase_VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, backbone: nn.Module, num_channels: int, name: str, return_interm_layers: bool):\n",
        "        super().__init__()\n",
        "        features = list(backbone.features.children())\n",
        "        if return_interm_layers:\n",
        "            if name == 'vgg16_bn':\n",
        "                self.body1 = nn.Sequential(*features[:13])\n",
        "                self.body2 = nn.Sequential(*features[13:23])\n",
        "                self.body3 = nn.Sequential(*features[23:33])\n",
        "                self.body4 = nn.Sequential(*features[33:43])\n",
        "            else:\n",
        "                self.body1 = nn.Sequential(*features[:9])\n",
        "                self.body2 = nn.Sequential(*features[9:16])\n",
        "                self.body3 = nn.Sequential(*features[16:23])\n",
        "                self.body4 = nn.Sequential(*features[23:30])\n",
        "        else:\n",
        "            if name == 'vgg16_bn':\n",
        "                self.body = nn.Sequential(*features[:44])  # 16x down-sample\n",
        "            elif name == 'vgg16':\n",
        "                self.body = nn.Sequential(*features[:30])  # 16x down-sample\n",
        "        self.num_channels = num_channels\n",
        "        self.return_interm_layers = return_interm_layers\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        out = []\n",
        "        if self.return_interm_layers:\n",
        "            xs = tensor_list.tensors\n",
        "            for _, layer in enumerate([self.body1, self.body2, self.body3, self.body4]):\n",
        "                xs = layer(xs)\n",
        "                out.append(xs)\n",
        "        else:\n",
        "            xs = self.body(tensor_list.tensors)\n",
        "            out.append(xs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Backbone_VGG(BackboneBase_VGG):\n",
        "    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n",
        "    def __init__(self, name: str, return_interm_layers: bool):\n",
        "        if name == 'vgg16_bn':\n",
        "            backbone = vgg16_bn(pretrained=True)\n",
        "        elif name == 'vgg16':\n",
        "            backbone = vgg16(pretrained=True)\n",
        "        num_channels = 512\n",
        "        super().__init__(backbone, num_channels, name, return_interm_layers)\n",
        "\n",
        "\n",
        "def build_backbone(args):\n",
        "\n",
        "    backbone = Backbone_VGG(args.backbone, True)\n",
        "\n",
        "    return backbone\n",
        "\n",
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self, num_features_in, num_anchor_points=4, feature_size=256):\n",
        "        super(RegressionModel, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.ReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "        self.act3 = nn.ReLU()\n",
        "\n",
        "        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "        self.act4 = nn.ReLU()\n",
        "\n",
        "        self.output = nn.Conv2d(feature_size, num_anchor_points * 2, kernel_size=3, padding=1)\n",
        "    # sub-branch forward\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.act1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.act2(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.act3(out)\n",
        "\n",
        "        out = self.conv4(out)\n",
        "        out = self.act4(out)\n",
        "\n",
        "        out = self.output(out)\n",
        "\n",
        "        out = out.permute(0, 2, 3, 1)\n",
        "\n",
        "        return out.contiguous().view(out.shape[0], -1, 2)\n",
        "\n",
        "# the network frmawork of the classification branch\n",
        "class ClassificationModel(nn.Module):\n",
        "    def __init__(self, num_features_in, num_anchor_points=4, num_classes=80, prior=0.01, feature_size=256):\n",
        "        super(ClassificationModel, self).__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_anchor_points = num_anchor_points\n",
        "\n",
        "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.ReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "        self.act3 = nn.ReLU()\n",
        "\n",
        "        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "        self.act4 = nn.ReLU()\n",
        "\n",
        "        self.output = nn.Conv2d(feature_size, num_anchor_points * num_classes, kernel_size=3, padding=1)\n",
        "        self.output_act = nn.Sigmoid()\n",
        "    # sub-branch forward\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.act1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.act2(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.act3(out)\n",
        "\n",
        "        out = self.conv4(out)\n",
        "        out = self.act4(out)\n",
        "\n",
        "        out = self.output(out)\n",
        "\n",
        "        out1 = out.permute(0, 2, 3, 1)\n",
        "\n",
        "        batch_size, width, height, _ = out1.shape\n",
        "\n",
        "        out2 = out1.view(batch_size, width, height, self.num_anchor_points, self.num_classes)\n",
        "\n",
        "        return out2.contiguous().view(x.shape[0], -1, self.num_classes)\n",
        "\n",
        "# generate the reference points in grid layout\n",
        "def generate_anchor_points(stride=16, row=3, line=3):\n",
        "    row_step = stride / row\n",
        "    line_step = stride / line\n",
        "\n",
        "    shift_x = (np.arange(1, line + 1) - 0.5) * line_step - stride / 2\n",
        "    shift_y = (np.arange(1, row + 1) - 0.5) * row_step - stride / 2\n",
        "\n",
        "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
        "\n",
        "    anchor_points = np.vstack((\n",
        "        shift_x.ravel(), shift_y.ravel()\n",
        "    )).transpose()\n",
        "\n",
        "    return anchor_points\n",
        "# shift the meta-anchor to get an acnhor points\n",
        "def shift(shape, stride, anchor_points):\n",
        "    shift_x = (np.arange(0, shape[1]) + 0.5) * stride\n",
        "    shift_y = (np.arange(0, shape[0]) + 0.5) * stride\n",
        "\n",
        "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
        "\n",
        "    shifts = np.vstack((\n",
        "        shift_x.ravel(), shift_y.ravel()\n",
        "    )).transpose()\n",
        "\n",
        "    A = anchor_points.shape[0]\n",
        "    K = shifts.shape[0]\n",
        "    all_anchor_points = (anchor_points.reshape((1, A, 2)) + shifts.reshape((1, K, 2)).transpose((1, 0, 2)))\n",
        "    all_anchor_points = all_anchor_points.reshape((K * A, 2))\n",
        "\n",
        "    return all_anchor_points\n",
        "\n",
        "# this class generate all reference points on all pyramid levels\n",
        "class AnchorPoints(nn.Module):\n",
        "    def __init__(self, pyramid_levels=None, strides=None, row=3, line=3):\n",
        "        super(AnchorPoints, self).__init__()\n",
        "\n",
        "        if pyramid_levels is None:\n",
        "            self.pyramid_levels = [3, 4, 5, 6, 7]\n",
        "        else:\n",
        "            self.pyramid_levels = pyramid_levels\n",
        "\n",
        "        if strides is None:\n",
        "            self.strides = [2 ** x for x in self.pyramid_levels]\n",
        "\n",
        "        self.row = row\n",
        "        self.line = line\n",
        "\n",
        "    def forward(self, image):\n",
        "        image_shape = image.tensors.shape[2:]\n",
        "        image_shape = np.array(image_shape)\n",
        "        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels]\n",
        "\n",
        "        all_anchor_points = np.zeros((0, 2)).astype(np.float32)\n",
        "        # get reference points for each level\n",
        "        for idx, p in enumerate(self.pyramid_levels):\n",
        "            anchor_points = generate_anchor_points(self.strides[idx], row=self.row, line=self.line)\n",
        "            shifted_anchor_points = shift(image_shapes[idx], self.strides[idx], anchor_points)\n",
        "            all_anchor_points = np.append(all_anchor_points, shifted_anchor_points, axis=0)\n",
        "\n",
        "        all_anchor_points = np.expand_dims(all_anchor_points, axis=0)\n",
        "        # send reference points to device\n",
        "        if torch.cuda.is_available():\n",
        "            return torch.from_numpy(all_anchor_points.astype(np.float32)).cuda()\n",
        "        else:\n",
        "            return torch.from_numpy(all_anchor_points.astype(np.float32))\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, C3_size, C4_size, C5_size, feature_size=256):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # upsample C5 to get P5 from the FPN paper\n",
        "        self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
        "        self.P5_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # add P5 elementwise to C4\n",
        "        self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
        "        self.P4_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # add P4 elementwise to C3\n",
        "        self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
        "        self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        C3, C4, C5 = inputs\n",
        "\n",
        "        P5_x = self.P5_1(C5)\n",
        "        P5_upsampled_x = self.P5_upsampled(P5_x)\n",
        "        P5_x = self.P5_2(P5_x)\n",
        "\n",
        "        P4_x = self.P4_1(C4)\n",
        "        P4_x = P5_upsampled_x + P4_x\n",
        "        P4_upsampled_x = self.P4_upsampled(P4_x)\n",
        "        P4_x = self.P4_2(P4_x)\n",
        "\n",
        "        P3_x = self.P3_1(C3)\n",
        "        P3_x = P3_x + P4_upsampled_x\n",
        "        P3_x = self.P3_2(P3_x)\n",
        "\n",
        "        return [P3_x, P4_x, P5_x]\n",
        "\n",
        "# the defenition of the P2PNet model\n",
        "class P2PNet(nn.Module):\n",
        "    def __init__(self, backbone, row=2, line=2):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.num_classes = 2\n",
        "        # the number of all anchor points\n",
        "        num_anchor_points = row * line\n",
        "\n",
        "        self.regression = RegressionModel(num_features_in=256, num_anchor_points=num_anchor_points)\n",
        "        self.classification = ClassificationModel(num_features_in=256, \\\n",
        "                                            num_classes=self.num_classes, \\\n",
        "                                            num_anchor_points=num_anchor_points)\n",
        "\n",
        "        self.anchor_points = AnchorPoints(pyramid_levels=[4,], row=row, line=line)\n",
        "\n",
        "        self.fpn = Decoder(256, 512, 512)\n",
        "\n",
        "    def forward(self, samples: NestedTensor):\n",
        "        # get the backbone features\n",
        "        features = self.backbone(samples)\n",
        "        # forward the feature pyramid\n",
        "        features_fpn = self.fpn([features[1], features[2], features[3]])\n",
        "\n",
        "        batch_size = features[0].shape[0]\n",
        "        # run the regression and classification branch\n",
        "        regression = self.regression(features_fpn[1]) * 100\n",
        "        classification = self.classification(features_fpn[1])\n",
        "\n",
        "        anchor_points = self.anchor_points(samples).repeat(batch_size, 1, 1)\n",
        "        # decode the points as prediction\n",
        "        output_coord = regression + anchor_points\n",
        "        output_class = classification\n",
        "        out = {'pred_logits': output_class, 'pred_points': output_coord}\n",
        "\n",
        "        return out\n",
        "\n",
        "def build(args, training=False):\n",
        "    # treats persons as a single class\n",
        "    num_classes = 1\n",
        "\n",
        "    backbone = build_backbone(args)\n",
        "    model = P2PNet(backbone, args.row, args.line)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o35jmzFcBjmG"
      },
      "source": [
        "## 4. Load Model and Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wFMZpZIUBjmH",
        "outputId": "4bbc3605-3502-4e8b-ea72-e1f6ce671d87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "P2PNet(\n",
              "  (backbone): Backbone_VGG(\n",
              "    (body1): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (9): ReLU(inplace=True)\n",
              "      (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (12): ReLU(inplace=True)\n",
              "    )\n",
              "    (body2): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (6): ReLU(inplace=True)\n",
              "      (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (9): ReLU(inplace=True)\n",
              "    )\n",
              "    (body3): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (6): ReLU(inplace=True)\n",
              "      (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (9): ReLU(inplace=True)\n",
              "    )\n",
              "    (body4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (6): ReLU(inplace=True)\n",
              "      (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (9): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (regression): RegressionModel(\n",
              "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (act1): ReLU()\n",
              "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (act2): ReLU()\n",
              "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (act3): ReLU()\n",
              "    (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (act4): ReLU()\n",
              "    (output): Conv2d(256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              "  (classification): ClassificationModel(\n",
              "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (act1): ReLU()\n",
              "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (act2): ReLU()\n",
              "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (act3): ReLU()\n",
              "    (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (act4): ReLU()\n",
              "    (output): Conv2d(256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (output_act): Sigmoid()\n",
              "  )\n",
              "  (anchor_points): AnchorPoints()\n",
              "  (fpn): Decoder(\n",
              "    (P5_1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (P5_upsampled): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    (P5_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (P4_1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (P4_upsampled): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    (P4_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (P3_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (P3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "args = edict({\n",
        "    'backbone': 'vgg16_bn',\n",
        "    'row': 2,\n",
        "    'line': 2,\n",
        "})\n",
        "\n",
        "model = build(args, training=False)\n",
        "model.to(DEVICE)\n",
        "if WEIGHTS_PATH:\n",
        "    checkpoint = torch.load(WEIGHTS_PATH, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQsl8YnZBjmH"
      },
      "source": [
        "## 5. Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wvlj0oGqBjmI"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms as standard_transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import glob\n",
        "import math\n",
        "\n",
        "def load_data(img_path, gt_path):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    # Load ground truth points\n",
        "    points = []\n",
        "    if os.path.exists(gt_path):\n",
        "        with open(gt_path) as f_label:\n",
        "            for line in f_label:\n",
        "                x, y = map(float , line.strip().split(' '))\n",
        "                points.append([x, y])\n",
        "    points = np.array(points)\n",
        "\n",
        "    return img, points\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, image_dir, gt_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.gt_dir = gt_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = sorted(glob.glob(os.path.join(image_dir, '*.jpg'))) # Assuming .jpg images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        img_name = os.path.basename(img_path)\n",
        "        gt_path = os.path.join(self.gt_dir, img_name.replace('.jpg', '.txt')) # Assuming .txt ground truth\n",
        "\n",
        "        img, points = load_data(img_path, gt_path)\n",
        "\n",
        "        original_width, original_height = img.size\n",
        "        # Round the size to be a multiple of 128, similar to run_test.py\n",
        "        new_width = math.floor(original_width / 128) * 128\n",
        "        new_height = math.floor(original_height / 128) * 128\n",
        "        if new_width == 0: new_width = 128\n",
        "        if new_height == 0: new_height = 128\n",
        "\n",
        "        img = img.resize((new_width, new_height), Image.LANCZOS)\n",
        "        if len(points) > 0:\n",
        "            points[:, 0] = points[:, 0] * (new_width / original_width)\n",
        "            points[:, 1] = points[:, 1] * (new_height / original_height)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        target = {'points': torch.from_numpy(points).float(), 'labels': torch.ones(len(points)).long()}\n",
        "\n",
        "        return img, target\n",
        "\n",
        "def collate_fn_crowd(batch):\n",
        "    batch_new = []\n",
        "    for b_img, b_target in batch:\n",
        "        # For single images, ensure consistent dimensionality\n",
        "        if b_img.ndim == 3:\n",
        "            b_img = b_img.unsqueeze(0)  # Add batch dimension\n",
        "        # The model expects target points to be a list of dicts, even for batch size 1\n",
        "        if 'points' in b_target:\n",
        "            b_target['point'] = b_target.pop('points') # Rename key for compatibility\n",
        "        if len(b_target['point'].shape) == 1 and b_target['point'].shape[0] == 0:\n",
        "            # Handle empty points case\n",
        "            b_target['point'] = torch.empty((0, 2), dtype=torch.float32)\n",
        "            b_target['labels'] = torch.empty((0,), dtype=torch.int64)\n",
        "        else:\n",
        "            # Squeeze to remove batch dimension if it exists and is 1\n",
        "            b_target['point'] = b_target['point'].squeeze(0) if b_target['point'].ndim == 3 else b_target['point']\n",
        "            b_target['labels'] = b_target['labels'].squeeze(0) if b_target['labels'].ndim == 2 else b_target['labels']\n",
        "        batch_new.append((b_img.squeeze(0), b_target)) # Remove initial batch dim for NestedTensor conversion\n",
        "\n",
        "    batch_imgs, batch_targets = list(zip(*batch_new))\n",
        "\n",
        "    # nested_tensor_from_tensor_list expects a list of tensors\n",
        "    batch_imgs_nested = nested_tensor_from_tensor_list(list(batch_imgs))\n",
        "\n",
        "    return batch_imgs_nested, list(batch_targets)\n",
        "\n",
        "# Define transformations\n",
        "transform = standard_transforms.Compose([\n",
        "    standard_transforms.ToTensor(),\n",
        "    standard_transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = CustomDataset(IMAGE_PATH, GT_PATH, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2, collate_fn=collate_fn_crowd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2_YzMu0BjmJ"
      },
      "source": [
        "## 6. Testing and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "CJSVubwXBjmJ",
        "outputId": "1f2f9d85-126d-4361-9786-104babc88b11"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (196608) must match the size of tensor b (49152) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2287396564.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moutputs_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred_logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3700554357.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0manchor_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m# decode the points as prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0moutput_coord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregression\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0manchor_points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0moutput_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'pred_logits'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pred_points'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput_coord\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (196608) must match the size of tensor b (49152) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "from torchvision.transforms.functional import to_pil_image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mae = 0\n",
        "mse = 0\n",
        "results = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (samples, targets) in enumerate(dataloader):\n",
        "        samples = samples.to(DEVICE)\n",
        "        outputs = model(samples)\n",
        "\n",
        "        outputs_scores = torch.nn.functional.softmax(outputs['pred_logits'], -1)[:, :, 1][0]\n",
        "        outputs_points = outputs['pred_points'][0]\n",
        "\n",
        "        threshold = 0.5\n",
        "        # filter the predictions\n",
        "        points = outputs_points[outputs_scores > threshold].detach().cpu().numpy().tolist()\n",
        "        predict_cnt = int((outputs_scores > threshold).sum())\n",
        "\n",
        "        gt_cnt = len(targets[0]['point'])\n",
        "\n",
        "        mae += abs(predict_cnt - gt_cnt)\n",
        "        mse += (predict_cnt - gt_cnt) * (predict_cnt - gt_cnt)\n",
        "\n",
        "        # Denormalize image for visualization\n",
        "        inv_normalize = standard_transforms.Normalize(\n",
        "            mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "            std=[1/0.229, 1/0.224, 1/0.225]\n",
        "        )\n",
        "        original_img_tensor = inv_normalize(samples.tensors[0].cpu())\n",
        "        original_img = to_pil_image(original_img_tensor)\n",
        "\n",
        "        results.append({\n",
        "            'image': original_img,\n",
        "            'predicted_points': points,\n",
        "            'predicted_count': predict_cnt,\n",
        "            'ground_truth_points': targets[0]['point'].cpu().numpy().tolist(),\n",
        "            'ground_truth_count': gt_cnt\n",
        "        })\n",
        "\n",
        "mae = mae / len(dataloader)\n",
        "mse = torch.sqrt(mse / len(dataloader))\n",
        "\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"MSE: {mse:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATSqCsPIBjmJ"
      },
      "source": [
        "## 7. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WQzOn36BjmJ"
      },
      "outputs": [],
      "source": [
        "def plot_results(result, image_id=0):\n",
        "    img_to_draw = np.array(result[image_id]['image'])\n",
        "    img_to_draw_pred = img_to_draw.copy()\n",
        "    img_to_draw_gt = img_to_draw.copy()\n",
        "\n",
        "    # Draw predicted points\n",
        "    for p in result[image_id]['predicted_points']:\n",
        "        cv2.circle(img_to_draw_pred, (int(p[0]), int(p[1])), 2, (0, 0, 255), -1)\n",
        "\n",
        "    # Draw ground truth points\n",
        "    for p in result[image_id]['ground_truth_points']:\n",
        "        cv2.circle(img_to_draw_gt, (int(p[0]), int(p[1])), 2, (0, 255, 0), -1)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "    axes[0].imshow(img_to_draw_pred)\n",
        "    axes[0].set_title(f\"Predicted Count: {result[image_id]['predicted_count']}\")\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(img_to_draw_gt)\n",
        "    axes[1].set_title(f\"Ground Truth Count: {result[image_id]['ground_truth_count']}\")\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example usage: Plot the first image's results\n",
        "if len(results) > 0:\n",
        "    plot_results(results, image_id=0)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}